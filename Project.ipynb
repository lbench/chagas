{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0bcf71",
   "metadata": {},
   "source": [
    "# CSE676 Project SLL for 12-lead ECG\n",
    "The code was adapted from https://github.com/tmehari/ecg-selfsupervised/tree/main?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1a8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from helper_code2 import *\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e0529",
   "metadata": {},
   "source": [
    "transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4da933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeOut:\n",
    "    \"\"\"Set random segment to 0. Expect Input is Tensor in (B,C,T) form. Output is Tensor in (B,C,T) form.\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_ratio_range=[0.0, 0.5]):\n",
    "        self.crop_ratio_range = crop_ratio_range\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        data, label = sample\n",
    "        data = data.clone()\n",
    "        timesteps = data.shape[-1]\n",
    "        crop_ratio = random.uniform(*self.crop_ratio_range)\n",
    "        crop_timesteps = int(crop_ratio*timesteps)\n",
    "        start_idx = random.randint(0, timesteps - crop_timesteps-1)\n",
    "        if data.dim() == 3:\n",
    "            data[:, :, start_idx:start_idx+crop_timesteps] = 0\n",
    "        else:\n",
    "            data[:, start_idx:start_idx+crop_timesteps] = 0\n",
    "        return data, label\n",
    "    \n",
    "class RandomResizeCrop:\n",
    "    \"\"\"Random crop and resize to original size. Input is Tensor in (B,C,T) form. Output is Tensor in (B,C,T) form\n",
    "    \"\"\"\n",
    "    def __init__(self, crop_ratio_range=[0.5, 1.0], output_size=4096):\n",
    "        self.crop_ratio_range = crop_ratio_range\n",
    "        self.output_size=output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        data, label = sample\n",
    "        timesteps = data.shape[-1]\n",
    "        crop_ratio = random.uniform(*self.crop_ratio_range)\n",
    "        crop_timesteps = int(crop_ratio*timesteps)\n",
    "        start = random.randint(0, timesteps - crop_timesteps-1)\n",
    "        if data.dim() == 3:\n",
    "            cropped_data = data[:, :, start: start + crop_timesteps]\n",
    "            resized = F.interpolate(cropped_data, size=self.output_size, mode='linear')\n",
    "            return resized, label\n",
    "        else:\n",
    "            cropped_data = data[:, start: start + crop_timesteps]\n",
    "            resized = F.interpolate(cropped_data.unsqueeze(0), size=self.output_size, mode='linear')\n",
    "            return resized.squeeze(), label\n",
    "    \n",
    "class RandomTransformation:\n",
    "    \"\"\"Generate augmentated data.\n",
    "    \"\"\"\n",
    "    def __init__(self, to_range=[0.0, 0.5], rrc_range=[0.5, 1.0]):\n",
    "        self.to = TimeOut(to_range)\n",
    "        self.rrc = RandomResizeCrop(rrc_range)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        z1 = self.to(self.rrc(x))\n",
    "        z2 = self.to(self.rrc(x))\n",
    "        return z1, z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee92307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XResBlock1d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1d1 = nn.Conv1d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.conv1d2 = nn.Conv1d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stride != 1 or in_channel != out_channel:\n",
    "            self.shorcut = nn.Sequential(\n",
    "                nn.AvgPool1d(kernel_size=2, stride=stride, ceil_mode=True),\n",
    "                nn.Conv1d(in_channel, out_channel, kernel_size=1),\n",
    "                nn.BatchNorm1d(out_channel)\n",
    "            )\n",
    "        else:\n",
    "            self.shorcut = nn.Identity()\n",
    "        nn.init.constant_(self.bn2.weight, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.relu(self.bn1(self.conv1d1(x)))\n",
    "        output = self.bn2(self.conv1d2(output))\n",
    "        output += self.shorcut(x)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "    \n",
    "class XResNet18(nn.Module):\n",
    "    def __init__(self, in_channel=12, out_channel=64, layers=[2, 2, 2, 2]):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.stem_pool = nn.MaxPool1d(3,2, padding=1)\n",
    "        \n",
    "        self.block1 = self.make_layer(64, 64, layers[0])\n",
    "        self.block2 = self.make_layer(64, 128, layers[1], stride=2)\n",
    "        self.block3 = self.make_layer(128, 256, layers[2], stride=2)\n",
    "        self.block4 = self.make_layer(256, 512, layers[3], stride=2)\n",
    "        \n",
    "        # Projector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.projection = nn.Sequential(nn.Linear(512, 2048),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(2048, out_channel))\n",
    "        \n",
    "    def make_layer(self, in_channel, out_channel, n_block, stride=1):\n",
    "        blocks = [XResBlock1d(in_channel, out_channel, stride=stride)]\n",
    "        for _ in range(n_block-1):\n",
    "            blocks.append(XResBlock1d(out_channel, out_channel, stride=1))\n",
    "        return nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward_encoder(self, x):\n",
    "        out = self.stem_pool(self.stem(x))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.avgpool(out)\n",
    "        return out.squeeze(-1)\n",
    "    \n",
    "    def forward_projection(self, feature):\n",
    "        out = self.projection(feature)\n",
    "        return F.normalize(out, dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feature = self.forward_encoder(x)\n",
    "        out = self.forward_projection(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5511b8",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4520cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, array):\n",
    "        return torch.from_numpy(array).T.float() # (T, C) -> (C, T)\n",
    "\n",
    "\n",
    "class NormalizeECG:\n",
    "\n",
    "    def __call__(self, sample, eps=1e-7):\n",
    "        mean = sample.mean(dim=0, keepdim=True)\n",
    "        std = sample.std(dim=0, keepdim=True)\n",
    "        result = (sample - mean) / (std + eps)\n",
    "        return result\n",
    "\n",
    "class PadECG:\n",
    "\n",
    "    def __init__(self, pad_to=4096):\n",
    "        self.pad_to = pad_to\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if sample.shape[-1] >= self.pad_to:\n",
    "            return sample[:, :self.pad_to] \n",
    "        else:\n",
    "            padding = (0, self.pad_to - sample.shape[1], 0, 0)\n",
    "            data = F.pad(sample, padding, \"constant\", 0)\n",
    "            return data\n",
    "        \n",
    "class ResizeECG:\n",
    "    def __init__(self, out_size=4096):\n",
    "        self.out_size = out_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        if sample.shape[-1] >= self.out_size:\n",
    "            return sample[:, :self.out_size]\n",
    "        else:\n",
    "            resized = F.interpolate(sample.unsqueeze(0), size=self.out_size, mode='linear')\n",
    "            return resized.squeeze()\n",
    "\n",
    "\n",
    "class FolderDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, min_len=800, upsampling=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder (str): Path to the folder containing the .dat and .hea pairs.\n",
    "        \"\"\"\n",
    "        self.folder = folder\n",
    "        self.min_len = min_len\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.record_paths, self.labels = self.find_records()\n",
    "        self.remove_short()\n",
    "        self.N_pos = sum(np.array(self.labels)==1)\n",
    "        self.N_neg = sum(np.array(self.labels)==0)\n",
    "        if upsampling:\n",
    "            self.upsampling()\n",
    "        \n",
    "    def upsampling(self):\n",
    "        pos_indices = [i for i, label in enumerate(self.labels) if label == 1]\n",
    "        while sum(np.array(self.labels)==1)<sum(np.array(self.labels)==0):\n",
    "            sampled_indice = random.choices(pos_indices)\n",
    "            self.labels.append(1)\n",
    "            self.record_paths.append(self.record_paths[sampled_indice[0]])\n",
    "\n",
    "    def find_records(self):\n",
    "        root = Path(self.folder)\n",
    "\n",
    "        records = []\n",
    "        for p in root.rglob('*.dat'):\n",
    "            p = p.with_suffix('')\n",
    "            header = load_header(p)\n",
    "            label = get_label(header)\n",
    "            records.append([p, label])\n",
    "\n",
    "        paths, labels = zip(*records)\n",
    "        return list(paths), list(labels)\n",
    "\n",
    "\n",
    "    def remove_short(self):\n",
    "        i = 0\n",
    "        while i < len(self.record_paths):\n",
    "            path = self.record_paths[i]\n",
    "            signal, fields = load_signals(str(path))\n",
    "            signal_len = signal.shape[0]\n",
    "            if signal_len < self.min_len:\n",
    "                self.record_paths.pop(i)\n",
    "                self.labels.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.record_paths[idx]\n",
    "        signal, fields = load_signals(record)\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, self.labels[idx]\n",
    "    \n",
    "    def get_weight(self):\n",
    "        return self.N_neg / self.N_pos\n",
    "    \n",
    "    def get_n_pos(self):\n",
    "        return sum(np.array(self.labels)==1)\n",
    "    \n",
    "    def get_n_neg(self):\n",
    "        return sum(np.array(self.labels)==0)\n",
    "    \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, path, transformation=None, augmentation=None, batchsize=64):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batchsize = batchsize\n",
    "        self.transformation = transformation\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = FolderDataset(self.path, transform=self.transformation)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9094d0",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c36c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/drive/1UK8BD3xvTpuSj75blz8hThfXksh19YbA?usp=sharing#scrollTo=GBNm6bbDT9J3\n",
    "def nt_xent_loss(out_1, out_2, temperature=0.5, eps=1e-6):\n",
    "    out = torch.cat([out_1, out_2], dim=0)\n",
    "    n_samples = len(out)\n",
    "    \n",
    "    cov = torch.mm(out, out.t().contiguous())\n",
    "    sim = torch.exp(cov / temperature)\n",
    "    \n",
    "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
    "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "    \n",
    "    # Positive similarity\n",
    "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "    pos = torch.cat([pos, pos], dim=0)\n",
    "\n",
    "    loss = -torch.log(pos / (neg + eps)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c0984",
   "metadata": {},
   "source": [
    "pl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84b962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLModule(pl.LightningModule):\n",
    "    def __init__(self, enconder, loss_fn=None, lr=1e-3, temperature=0.5, epochs=20):\n",
    "        super().__init__()\n",
    "        self.encoder = enconder\n",
    "        self.lr = lr\n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x1, _), (x2, _) = self.trainer.datamodule.augmentation((batch))\n",
    "        z1 = self.encoder(x1)\n",
    "        z2 = self.encoder(x2)\n",
    "        loss = self.loss_fn(z1, z2, temperature=self.temperature)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ac7b7",
   "metadata": {},
   "source": [
    "### Pretrain phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ca3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = Compose([ToTensor(), NormalizeECG(), ResizeECG()])\n",
    "augmentation = RandomTransformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811d3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr=5e-3\n",
    "out_channel = 256\n",
    "layers = [3, 4,  6, 3]\n",
    "temperature = 0.2\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Tools\\Anaconda\\envs\\gymenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(path=\"./code15_output/\", transformation=transformation, augmentation=augmentation, batchsize=bs)\n",
    "encoder = XResNet18(out_channel=out_channel, layers=layers)\n",
    "model = SSLModule(enconder=encoder, loss_fn=nt_xent_loss, lr=lr, temperature=temperature, epochs=epochs)\n",
    "trainer = pl.Trainer(max_epochs=epochs, accelerator='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc84cdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | encoder | XResNet18 | 8.8 M  | train\n",
      "----------------------------------------------\n",
      "8.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.8 M     Total params\n",
      "35.248    Total estimated model params size (MB)\n",
      "140       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\Tools\\Anaconda\\envs\\gymenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 312/312 [03:36<00:00,  1.44it/s, v_num=1, train_loss_step=0.639, train_loss_epoch=1.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 312/312 [03:37<00:00,  1.44it/s, v_num=1, train_loss_step=0.639, train_loss_epoch=1.050]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326fd94",
   "metadata": {},
   "source": [
    "v1: epochs = 50\n",
    "lr=1e-2\n",
    "out_channel = 256\n",
    "layers = [3, 4,  6, 3]\n",
    "temperature = 0.3\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4802d",
   "metadata": {},
   "source": [
    "v2: epochs = 100\n",
    "lr=5e-3\n",
    "out_channel = 256\n",
    "layers = [3, 4,  6, 3]\n",
    "temperature = 0.2\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e325e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "log = pd.read_csv(\"lightning_logs/version_1/metrics.csv\")\n",
    "log = log[[\"epoch\", \"train_loss_epoch\"]].dropna()\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(log[\"epoch\"], log[\"train_loss_epoch\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.encoder.state_dict(), \"encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ca1e6",
   "metadata": {},
   "source": [
    "### Finetune phase (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(pl.LightningModule):\n",
    "    def __init__(self, encoder, out_dim=1, lr=1e-3, epochs=10, pos_weight=1, linear=False, frozen=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.lr = lr\n",
    "        if linear:\n",
    "            self.classifier = nn.Linear(512, out_dim)\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, out_dim)\n",
    "                )\n",
    "        self.epochs = epochs\n",
    "        self.best_val_loss = 1\n",
    "        self.frozen = frozen \n",
    "        \n",
    "        # Freeze encoder \n",
    "        if self.frozen:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False        \n",
    "            \n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.encoder.forward_encoder(x)\n",
    "        out = self.classifier(feature)\n",
    "        return out.squeeze(-1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = self.loss_fn(pred, y.float())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = self.loss_fn(pred, y.float())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = self.trainer.callback_metrics.get(\"val_loss\")\n",
    "        if loss < self.best_val_loss:\n",
    "            self.best_val_loss = loss\n",
    "            torch.save(self.classifier.state_dict(), \"Best_classifier.pt\")\n",
    "            if not self.frozen:\n",
    "                torch.save(self.encoder.state_dict(), \"Best_encoder.pt\")\n",
    "    \n",
    "class FinetuneDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_path, val_path, transformation=None, augmentation=None, batchsize=64, upsampling=False):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.batchsize = batchsize\n",
    "        self.transformation = transformation\n",
    "        self.augmentation = augmentation\n",
    "        self.upsampling = upsampling\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = FolderDataset(self.train_path, transform=self.transformation, upsampling=self.upsampling)\n",
    "        self.val_dataset = FolderDataset(self.val_path, transform=self.transformation)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batchsize, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if self.val_path:\n",
    "            return DataLoader(self.val_dataset, batch_size=self.batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "099d9198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = XResNet18(out_channel=out_channel, layers=layers)\n",
    "encoder.load_state_dict(torch.load(\"encoder_pretrain.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "34cbd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr=5e-4\n",
    "out_dim = 1\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a3cb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = FinetuneDataModule(train_path=\"./training_data/\", val_path='./val_data/', transformation=transformation, batchsize=bs, upsampling=False)\n",
    "train_loader.setup()\n",
    "pos_weight = train_loader.train_dataset.get_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c2fa299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "classifier = ClassifierModule(encoder=encoder, lr=lr, out_dim = out_dim, epochs=epochs, pos_weight=pos_weight, frozen=False)\n",
    "trainer = pl.Trainer(max_epochs=epochs, accelerator='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(classifier, datamodule=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84ccdb",
   "metadata": {},
   "source": [
    "V2 epochs = 10\n",
    "lr=1e-4\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "no schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4a578",
   "metadata": {},
   "source": [
    "V5 epochs = 10\n",
    "lr=5e-4\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb9df4",
   "metadata": {},
   "source": [
    "V6 epochs = 20\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a927b",
   "metadata": {},
   "source": [
    "V7 epochs = 20\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular\n",
    "pos_weight = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c30922",
   "metadata": {},
   "source": [
    "V8 epochs = 20\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular\n",
    "upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbb77f",
   "metadata": {},
   "source": [
    "V9 epochs = 10\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular\n",
    "upsampling\n",
    "non_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99ef02",
   "metadata": {},
   "source": [
    "V10 epochs = 20\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular\n",
    "weighted\n",
    "non_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1370405",
   "metadata": {},
   "source": [
    "V12 epochs = 20\n",
    "lr=1e-3\n",
    "out_dim = 1\n",
    "bs = 128\n",
    "with schedular\n",
    "weighted\n",
    "non_linear\n",
    "no freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
